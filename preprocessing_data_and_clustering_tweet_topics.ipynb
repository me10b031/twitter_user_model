{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bHGihPrKMAJa",
    "outputId": "e7da904c-4d0d-42d6-83ee-bab85a52d5a6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nejGa1C50IrM"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from preprocessing import tokenize, export_to_csv\n",
    "from gsdmm import MovieGroupProcess\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from IPython.display import clear_output\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword(text):\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def stemwords(text):    \n",
    "    ps = PorterStemmer()\n",
    "    text = text.lower().split()\n",
    "    stemmed_words = [ps.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pickle file is obatined by filtering english language users who have atleast 20 tweets on their home timeline\n",
    "#It is straightforward hence we generate a pickle file of filtered dataset and use here directly.\n",
    "\n",
    "file = open(\"filtered_english.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm',parser=False, entity=False)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "file = open(\"filtered_english.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "df.drop([\"engaging_user_id\",\"tweet_id\",\"tweet_timestamp\",\"reply_timestamp\",\"retweet_timestamp\",\"retweet_with_comment_timestamp\",\"like_timestamp\"], axis = 1)\n",
    "df['text_tokens']= df['text_tokens'].str.split('\\t').apply(lambda x:list(map(int, x)))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "df['text_tokens'] = df['text_tokens'].apply(lambda x:tokenizer.convert_ids_to_tokens(x))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv(s):\n",
    "    l = []\n",
    "    for i in range(len(s)):\n",
    "        if s[i][:2]!='##':\n",
    "            l.append(s[i])\n",
    "        else:\n",
    "            l[-1]=l[-1]+s[i][2:]\n",
    "    return l\n",
    "\n",
    "\n",
    "df['text_tokens'] = df['text_tokens'].apply(lambda x: conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens']= df['text_tokens'].apply(lambda x: \" \".join([y for y in x]))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "for r in ((\"[CLS]\", \"\"),(\"[SEP]\",\"\"),(\"[UNK]\",\"\"),(\"RT\",\"\")):\n",
    "    df['text_tokens'] = df['text_tokens'].apply(lambda x: x.replace(*r))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda x:' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda x: stemwords(x))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda x: stopword(x))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "\n",
    "#df.to_pickle('text_for_topics_1.pkl')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens'] = df['text_tokens'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['text_tokens'].tolist()\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(\"Voc size:\", n_terms)\n",
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=50, alpha=0.1, beta=0.1, n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model \n",
    "\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "\n",
    "start_time = time.time()\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "n_docs = len(docs)\n",
    "\n",
    "# Fit the model on the data given the chosen seeds\n",
    "y = mgp.fit(docs, n_terms)\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', sorted(doc_count))\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(distribution, top_index, num_words):\n",
    "    for topic in top_index:\n",
    "        pairs = sorted([(k, v) for k, v in distribution[topic].items()], key=lambda x: x[1], reverse=True)\n",
    "        a = pairs[:num_words]\n",
    "        c = [i[0] for i in a]\n",
    "        print (c)\n",
    "        print('.'*30)\n",
    "\n",
    "b = top_words(mgp.cluster_word_distribution, top_index, 1000)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens'] = df['text_tokens'].apply(lambda x: mgp.choose_best_label(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gsdmm_topics.pkl', \"wb\") as f:\n",
    "     pickle.dump(df, f)\n",
    "     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "product_choice_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
